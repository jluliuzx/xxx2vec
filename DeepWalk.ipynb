{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "import networkx as nx\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "from joblib import Parallel,delayed\n",
    "import random\n",
    "import itertools\n",
    "from gensim.models import Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def partition_num(num, workers):\n",
    "    if num % workers == 0:\n",
    "        return [num//workers]*workers\n",
    "    else:\n",
    "        return [num//workers]*workers + [num % workers]\n",
    "\n",
    "class RandomWalker:\n",
    "    def __init__(self,G,p=1,q=1):\n",
    "        self.G = G\n",
    "        self.p = p\n",
    "        self.q = q\n",
    "        \n",
    "    def deepwalk_walk(self,walk_length,start_node):\n",
    "        walk = [start_node]\n",
    "        \n",
    "        while len(walk) < walk_length:\n",
    "            cur = walk[-1]\n",
    "            cur_nbrs = list(self.G.neighbors(cur))\n",
    "            if len(cur_nbrs) > 0:\n",
    "                walk.append(random.choice(cur_nbrs))\n",
    "            else:\n",
    "                break\n",
    "        return walk\n",
    "    \n",
    "    def simulate_walks(self,num_walks,walk_length,workers=1,verbose=0):\n",
    "        G = self.G\n",
    "        \n",
    "        nodes = list(G.nodes())\n",
    "        \n",
    "        results = Parallel(n_jobs=workers,verbose=verbose,)(\n",
    "            delayed(self._simulate_walks)(nodes,num,walk_length)\n",
    "            for num in partition_num(num_walks,workers))\n",
    "        \n",
    "        walks = list(itertools.chain(*results))\n",
    "        \n",
    "        return walks\n",
    "    \n",
    "    def _simulate_walks(self,nodes,num_walks,walk_length,):\n",
    "        walks = []\n",
    "        for _ in range(num_walks):\n",
    "            random.shuffle(nodes)\n",
    "            for v in nodes:\n",
    "                if self.p == 1 and self.q == 1:\n",
    "                    walks.append(self.deepwalk_walk(\n",
    "                        walk_length=walk_length,start_node=v))\n",
    "                else:\n",
    "                    walks.append(self.node2vec_walk(\n",
    "                        walk_length=walk_length,start_node=v))\n",
    "        return walks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DeepWalk:\n",
    "    def __init__(self,graph,walk_length,num_walks,workers=1):\n",
    "        self.graph = graph\n",
    "        self.w2v_model = None\n",
    "        self._embeddings = {}\n",
    "        \n",
    "        self.walker = RandomWalker(graph,p=1,q=1,)\n",
    "        self.sentences = self.walker.simulate_walks(num_walks=num_walks,walk_length=walk_length,workers=workers,verbose=1)\n",
    "        \n",
    "    def train(self,embed_size=128,window_size=5,workers=3,iter=5,**kwargs):\n",
    "        kwargs[\"sentences\"] = self.sentences\n",
    "        kwargs[\"min_count\"] = kwargs.get(\"min_count\",0)\n",
    "        kwargs[\"size\"] = embed_size\n",
    "        kwargs[\"sg\"] = 1                # skip gram\n",
    "        kwargs[\"hs\"] = 1                # deepwalk use Hierarchical Softmax\n",
    "        kwargs[\"workers\"] = workers\n",
    "        kwargs[\"window\"] = window_size\n",
    "        kwargs[\"iter\"] = iter\n",
    "        \n",
    "        print(\"Learning embedding vectors...\")\n",
    "        model = Word2Vec(**kwargs)\n",
    "        print(\"Learning embedding vertors done!\")\n",
    "        \n",
    "        self.w2v_model = model\n",
    "        return model\n",
    "    \n",
    "    def get_embeddings(self,):\n",
    "        if self.w2v_model in None:\n",
    "            print(\"model not train\")\n",
    "            return{}\n",
    "        \n",
    "        self._embeddings = {}\n",
    "        for word in self.graph.nodes():\n",
    "            self._embeddings[word] = self.w2v_model.wv[word]\n",
    "        \n",
    "        return self._embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_node_label(filename,skip_head=False):\n",
    "    fin = open(filename,'r')\n",
    "    X = []\n",
    "    Y = []\n",
    "    while 1:\n",
    "        if skip_head:\n",
    "            fin_readline()\n",
    "        l = fin.readline()\n",
    "        if l == '':\n",
    "            break\n",
    "        vec = l.strip().split('')\n",
    "        X.append(vec[0])\n",
    "        Y.append(vec[1:])\n",
    "    fin.close()\n",
    "    return X,Y\n",
    "\n",
    "class TopKRanker(OneVsRestClassifier):\n",
    "    def predict(self,X,top_k_list):\n",
    "        probs = numpy.asarray(super(TopKRanker,self).predict_proba(X))\n",
    "        all_labels = []\n",
    "        for i,k in enumerate(top_k_list):\n",
    "            probs = probs[i,:]\n",
    "            labels = self.classes_[probs_.argsort()[-k:]].tolist()\n",
    "            probs_[:] = 0\n",
    "            probs_[labels] = 1\n",
    "            all_labels.append(probs_)\n",
    "        return numpy.asarray(all_labels)\n",
    "\n",
    "class Classifier(object):\n",
    "    \n",
    "    def __init__(self,embedding,clf):\n",
    "        self.embeddings = embeddings\n",
    "        self.clf = TopKRanker(clf)\n",
    "        self.binarizer = MultiLabelBinarizer(sparse_output=True)\n",
    "        \n",
    "    def train(self,X,Y,Y_all):\n",
    "        self.binarizer.fit(Y_all)\n",
    "        X_train = [self.embeddings[x] for x in X]\n",
    "        Y = self.binarizer.transform(Y)\n",
    "        self.clf.fit(X_train,Y)\n",
    "        \n",
    "    def evaluate(self,X,Y):\n",
    "        top_k_list = [len(l) for l in Y]\n",
    "        Y = self.predict(X,top_k_list)\n",
    "        Y = self.binarizer.transform(Y)\n",
    "        averages = [\"micro\",\"macro\",\"samples\",\"weighted\"]\n",
    "        results = {}\n",
    "        for average in averages:\n",
    "            results[average] = f1_score(Y,Y_,average=average)\n",
    "        results['acc'] = accuracy_score(Y,Y_)\n",
    "        print('------------------------------')\n",
    "        print(results)\n",
    "        return results\n",
    "        \n",
    "    def split_train_evaluate(self,X,Y,train_precent,seed=0):\n",
    "        state = numpy.random.get_state()\n",
    "        \n",
    "        training_size = int(train_precent * len(X))\n",
    "        numpy.random.seed(seed)\n",
    "        shuffle_indices = numpy.random.permutation(numpy.arange(len(X)))\n",
    "        X_train = [X[shuffle_indices[i]] for i in range(training_size)]\n",
    "        Y_train = [Y[shuffle_indices[i]] for i in range(training_size)]\n",
    "        X_test = [X[shuffle_indices[i]] for i in range(training_size,len(X))]\n",
    "        Y_test = [Y[shuffle_indices[i]] for i in range(training_size,len(X))]\n",
    "        \n",
    "        self.train(X_train,Y_train,Y)\n",
    "        numpy.random.set_state(state)\n",
    "        return  self.evaluate(X_test,Y_test)\n",
    "\n",
    "def evaluate_embeddings(embeddings):\n",
    "    X,Y = read_node_label('./data/wiki/wiki_labels.txt')\n",
    "    tr_frac = 0.8\n",
    "    print(\"Training classifier using {:.2f}% nodes...\".format(tr_frac * 100))\n",
    "    clf = Classifier(embeddings=embeddings,clf=LogisticRegression())\n",
    "    clf.split_train_evaluate(X,Y,tr_frac)\n",
    "    \n",
    "def plot_embeddings(embeddings,):\n",
    "    X,Y = read_node_label('../data/wiki/wiki_labels.txt')\n",
    "    \n",
    "    emb_list = []\n",
    "    for k in X:\n",
    "        emb_list.append(embeddings[k])\n",
    "    emb_list = np.array(emb_list)\n",
    "    \n",
    "    model = TSNE(n_components=2)\n",
    "    node_pos = model.fit_transform(emb_list)\n",
    "    \n",
    "    color_idx = {}\n",
    "    for i in range(len(x)):\n",
    "        color_idx.setdefault(Y[i][0],[])\n",
    "        color_idx[Y[i][0]].append(i)\n",
    "        \n",
    "    for c,idx in color_idx.items():\n",
    "        plt.scatter(node_pos[idx,0],node_pos[idx,1],label=c)\n",
    "    plt.legend()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    G = nx.read_edgelist('./data/wiki/Wiki_edgelist.txt',create_using=nx.DiGraph(),nodetype=None,data=[('weight',int)])\n",
    "    \n",
    "    model = DeepWalk(G,walk_length=10,num_walks=80,workers=1)\n",
    "    model.train(window_size=5,iter=3)\n",
    "    embeddings = model.get_embeddings()\n",
    "    \n",
    "    evaluate_embeddings(embeddings)\n",
    "    plot_embeddings(embeddings)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
